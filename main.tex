\documentclass[10pt,landscape,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[utopia,sfscaled]{mathdesign}
\usepackage{multicol}
\usepackage[top=2mm,bottom=2mm,left=2mm,right=2mm]{geometry}
\usepackage{lipsum}
\usepackage[framemethod=tikz]{mdframed}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{enumitem}

\setitemize{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0.2\baselineskip}

\begin{document}
\footnotesize
\begin{multicols*}{5}
\section{Validation}
% TODO: 2016_valid Q1 pseudocode

% TODO: different validation techniques

\section{Parametric \& Non-Parametric}
\begin{itemize}
    \item parametric
    \begin{itemize}
        \item fixed \# parameters
        \item more data doesn't help
    \end{itemize}
    \item non-parametric
    \begin{itemize}
        \item \# parameters grows with \(n\)
        \item more data \(\rightarrow \)  more complicated
    \end{itemize}
    \item examples:
    \begin{itemize}
        \item k-means (non-parameters)
        \item softmax multi-class classification (parametric)
        \item PCA (parametric)
        \item neural networks (parametric)
    \end{itemize}
\end{itemize}

\section{Data Standardization}
\begin{itemize}
    \item \(\mu \) = mean of \(X_j\) and \(y\), \(\sigma \) = std of \(X_j\) and \(y\)
    \item use the \(\mu \) and \(\sigma \) from training data on test data
\end{itemize}

\section{Math}
Objective functions and derivatives:
\begin{align*}
   f(x) &= \frac{1}{2}\sum\limits_{i=1}^{n} (w^\intercal x_i - y_i)^2 + \frac{\lambda}{2} \sum\limits_{j=1}^{d} w_j^2 \\
   &= \frac{1}{2} ||Xw-y||^2 + \frac{\lambda}{2} ||w||^2
\end{align*}
\begin{itemize}
    \item \(\sum\limits_{i=1}^{n} (w^\intercal x_i - y_i)^2 = \sum\limits_{i=1}^{n} r_i^2 = r^\intercal r = ||r||^2\)
    \item \(||v|| = \sqrt{\sum\limits_{j=1}^{d} v_j^2}\), \(u^\intercal v = \sum\limits_{j=1}^{d} u_j v_j\)
    \item \(||w||^2 = \sum\limits_{j=1}^{d} w_j^2 = \sum\limits_{j=1}^{d} w_j w_j = w^\intercal w \)
\end{itemize}
\begin{align*}
    f(w) &= \frac{1}{2} (Xw-y)^\intercal (Xw-y) + \frac{\lambda}{2} w^\intercal w \\
    &= \frac{1}{2} w^\intercal X^\intercal Xw - w^\intercal X^\intercal y - \frac{1}{2} y^\intercal y + \frac{\lambda}{2} w^\intercal w \\
    \nabla f(x) &= X^\intercal Xw - X^\intercal y + \lambda w = 0 \\
    & \rightarrow (X^\intercal X + \lambda I) w = X^\intercal y
\end{align*}
\begin{itemize}
    \item \(\nabla_w (c) = 0\), \(\nabla_w (w^\intercal b) = b\), \(\nabla_w (\frac{1}{2} w^\intercal A w = Aw\)
\end{itemize}
\begin{itemize}
    \item if \(f(x) = \cdots \frac{\lambda}{2} \sum\limits_{j=1}^{d} w_j v_j\), then \(f(x) = \cdots \lambda w^\intercal v\) and \(\nabla f(x) = X^\intercal Xw - X^\intercal y + \lambda v = 0\)
\end{itemize}
\begin{align*}
    f(x) &= \sum\limits_{i=1}^{n} z_i |w^\intercal x_i - y_i| + \lambda \max_j |w_j|
\end{align*}
\begin{align*}
    f(w) = ||Z(Xw - y)||_1 + \lambda ||w||_{\infty}
\end{align*}

\section{Some Tips}
\begin{itemize}
    \item \textbf{decision stump}: computer error for different stumps then choose the one with the least error
\end{itemize}

\end{multicols*}
\end{document}
